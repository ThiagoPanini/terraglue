*Fornecendo exemplos pr√°ticos de cen√°rios de utiliza√ß√£o do projeto*

## Table of Contents
- [Table of Contents](#table-of-contents)
- [Antes de come√ßar](#antes-de-come√ßar)
- [Cen√°rio 1: implementando seu pr√≥prio conjunto de dados](#cen√°rio-1-implementando-seu-pr√≥prio-conjunto-de-dados)
  - [Sobre os dados de exemplo (Brazilian E-Commerce)](#sobre-os-dados-de-exemplo-brazilian-e-commerce)
  - [Utilizando dados pr√≥prios](#utilizando-dados-pr√≥prios)
  - [Visualizando efeitos na conta AWS](#visualizando-efeitos-na-conta-aws)
- [Cen√°rio 2: implementando seu pr√≥prio job do Glue](#cen√°rio-2-implementando-seu-pr√≥prio-job-do-glue)
  - [Etapas para adapta√ß√£o da aplica√ß√£o](#etapas-para-adapta√ß√£o-da-aplica√ß√£o)
  - [Alterando par√¢metros do job](#alterando-par√¢metros-do-job)
  - [Modificando o dicion√°rio DATA\_DICT](#modificando-o-dicion√°rio-data_dict)
  - [Codificando novos m√©todos de transforma√ß√£o](#codificando-novos-m√©todos-de-transforma√ß√£o)
  - [Sequenciando passos no m√©todo run()](#sequenciando-passos-no-m√©todo-run)
  - [Visualizando resultados](#visualizando-resultados)
___

## Antes de come√ßar

> Antes de navegarmos por exemplos pr√°ticos d euso da solu√ß√£o, √© importante garantir que todas as etapas de prepara√ß√£o e instala√ß√£o foram cumpridas. Para maiores detalhes, o arquivo [GETTINGSTARTED.md](https://github.com/ThiagoPanini/terraglue/blob/develop/GETTINGSTARTED.md) contempla todo o processo necess√°rio de inicia√ß√£o. Adicionalmente, o arquivo [INFRA.md](https://github.com/ThiagoPanini/terraglue/blob/main/INFRA.md) cont√©m todas as explica√ß√µes sobre os recursos de infraestrutura provisionados ao usu√°rio. Por fim, o arquivo [APP.md](https://github.com/ThiagoPanini/terraglue/blob/main/APP.md) traz detalhes sobre os m√≥dulos e scripts pr√© codificados e entregues aos usu√°rios.

- [1. Documenta√ß√£o principal do projeto](https://github.com/ThiagoPanini/terraglue/tree/main)
- [2. Instala√ß√£o e primeiros passos](https://github.com/ThiagoPanini/terraglue/blob/main/GETTINGSTARTED.md) 
- [3. Infraestrutura provisionada](https://github.com/ThiagoPanini/terraglue/blob/main/INFRA.md) 
- [4. Uma proposta de padroniza√ß√£o de jobs Glue](https://github.com/ThiagoPanini/terraglue/blob/main/APP.md) 
- üëâ [5. Exemplos pr√°ticos de utiliza√ß√£o da solu√ß√£o](https://github.com/ThiagoPanini/terraglue/blob/main/EXAMPLES.md) *Voc√™ est√° aqui!*


Adicionalmente, √© v√°lido citar que esta documenta√ß√£o ser√° separada em diferentes **cen√°rios**, cada um trazendo √† tona uma poss√≠vel seara de aplica√ß√£o do **terraglue** de acordo com um prop√≥sito espec√≠fico. √â importante destacar que os cen√°rios contemplam desafios pr√≥prios e particulares, sendo direcionados para p√∫blicos espec√≠ficos que podem se beneficiar das funcionalidades deste projeto. Encontre aquele que mais fa√ßa sentido dentro de sua jornada de aprendizado e mergulhe fundo!

| üé¨ **Cen√°rio** | **üéØ P√∫blico alvo** |
| :-- | :-- |
| [#1 Implementando seu pr√≥prio conjunto de dados](#cen√°rio-1-implementando-seu-pr√≥prio-conjunto-de-dados) | Usu√°rios com conhecimentos b√°sicos |
| [#2 Implementando seu pr√≥prio job do Glue](#cen√°rio-2-implementando-seu-pr√≥prio-job-do-glue) | Usu√°rios com conhecimentos intermedi√°rios |

___

## Cen√°rio 1: implementando seu pr√≥prio conjunto de dados

Considerando que o usu√°rio, neste momento da jornada de consumo da documenta√ß√£o, tem uma no√ß√£o b√°sica sobre o funcionamento do **terraglue** como produto, o primeiro cen√°rio de utiliza√ß√£o pr√°tica a ser exemplificado envolve a adapta√ß√£o da solu√ß√£o para ingest√£o e cataloga√ß√£o de bases pr√≥prias a serem utilizadas, posteriormente, como insumos de jobs Glue a serem programados.

Este tipo de adapta√ß√£o √© fundamentalmente importante aos usu√°rios de todos os n√≠veis pois, a partir dela, √© poss√≠vel:

- üé≤ Utilizar amostras de bases produtivas para valida√ß√µes e testes na AWS
- üïπÔ∏è Simular um cen√°rio de desenvolvimento pr√≥ximo daquele encontrado em ambientes produtivos
- üßô‚Äç‚ôÇÔ∏è Acelerar o processo de desenvolvimento de *jobs* a partir do entendimento e utiliza√ß√£o de amostras pr√≥prias de dados
- üîç Executar consultas *ad-hoc* em dados catalogados automaticamente

Antes de iniciar o processo de adapta√ß√£o da solu√ß√£o para uso de dados pr√≥prios, √© preciso entender um pouco mais sobre os dados originalmente disponibilizados no reposit√≥rio.

### Sobre os dados de exemplo (Brazilian E-Commerce)

Antes de iniciar as discuss√µes desta se√ß√£o, √© imprescind√≠vel abordar, de forma resumida, alguns detalhes sobre o conjunto de dados fornecido por padr√£o no reposit√≥rio: trata-se de bases que contemplam o famoso dataset [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce).

Originalmente dispon√≠vel na plataforma [Kaggle](https://www.kaggle.com/), o referido conjunto contempla dados de vendas online no *e-commerce* brasileiro separados em diferentes arquivos de texto, cada um em seu respectivo dom√≠nio. Juntos, os arquivos podem ser utilizados e trabalhados para retirar os mais variados *insights* relacionados ao com√©rcio online.

<details>
  <summary>üé≤ Clique para visualizar o schema original dos dados</summary>
  <div align="left">
    <br><img src="https://github.com/ThiagoPanini/terraglue/blob/develop/docs/imgs/examples-cenario03-schema-br-ecommerce.png?raw=true" alt="br-ecommerce-schema">
</div>
</details>

Visando proporcionar uma maior simplicidade no exemplo de gera√ß√£o de SoT, apenas alguns arquivos do conjunto de dados Brazilian E-Commerce foram selecionados como SoRs do projeto, sendo eles:

| üóÇÔ∏è **Arquivo** | üìù **Descri√ß√£o** |
| :-- | :-- |
| `orders.csv` | Contempla dados de pedidos realizados online |
| `customers.csv` | Contempla dados cadastrais dos clientes que realizaram os pedidos online |
| `payments.csv` | Contempla dados dos pagamentos utilizados para quita√ß√£o dos pedidos realizados |
| `reviews.csv` | Contempla dados das revis√µes, nota e coment√°rios deixados por clientes para os pedidos realizados |

Assim, os conjuntos citados ent√£o s√£o disponibilizados localmente em uma estrutura hier√°rquica de pastas que simula uma organiza√ß√£o de dados em um ambiente Data Lake no formato `db/tbl/file`, sendo esta uma din√¢mica **mandat√≥ria** para o sucesso de implanta√ß√£o de conjuntos pr√≥prios de dados.

### Utilizando dados pr√≥prios

Conhecidos os arquivos que fazem parte do reposit√≥rio em seu modo natural, o usu√°rio agora poder√° adaptar todo o processo de ingest√£o e cataloga√ß√£o proporcionado no **terraglue** para seus pr√≥prios conjuntos.

Em um primeiro momento, √© extremamente ressaltar algumas premissas e limita√ß√µes do processo de cataloga√ß√£o de dados no Data Catalog da conta AWS:

1. Somente arquivos `csv` poder√£o ser utilizados
2. Os arquivos `csv` devem possuir o *header* na primeira linha
3. A estrutura hier√°rquica deve seguir o modelo `db/tbl/file` a partir do diret√≥rio `data/` do reposit√≥rio

> üö® Avaliar as premissas acima √© de suma import√¢ncia pois, em seus detalhes t√©cnicos de constru√ß√£o, o **terraglue** considera a aplica√ß√£o de fun√ß√µes do Terraform para iterar sobre os diret√≥rios presentes em `data/`, realizar a leitura da primeira linha dos arquivos CSV para extra√ß√£o dos atributos e cataloga√ß√£o no Data Catalog. Sem o cumprimento das premissas, as fun√ß√µes do Terraform ir√£o retornar erro e o fluxo n√£o ser√° implantado conforme esperado pelo usu√°rio. Para maiores detalhes, consulte a documenta√ß√£o da fun√ß√£o [fileset()](https://developer.hashicorp.com/terraform/language/functions/fileset) do Terraform.

Endere√ßado este ponto, os exemplos ilustrados a seguir simulam a obten√ß√£o de novos conjuntos de dados a serem utilizados no processo de ingest√£o e cataloga√ß√£o em substitui√ß√£o aos dados originais do dataset Brazilian E-Commerce fornecidos como padr√£o.

Como um primeiro passo, o usu√°rio pode navegar at√© o reposit√≥rio do **terraglue** clonado localmente e executar o comando abaixo para remover todo o cont√©udo presente em `data/` no reposit√≥rio do projeto. Tamb√©m √© poss√≠vel excluir os arquivos e diret√≥rios manualmente, se preferir.

```bash
# Removendo todos os arquivos de /data
rm -r data/*
```

Com o diret√≥rio `/data` agora vazio, basta obter os novos dados a serem utilizados e organiz√°-los na estrutura adequada. Como exemplo, utilizarei dados do [naufr√°gio do Titanic](https://www.kaggle.com/competitions/titanic/data) previamente baixados e armazenados no diret√≥rio `~/Downloads` com o nome `titanic.csv`. Antes de realizar a movimenta√ß√£o do arquivo, √© importante criar toda a estrutura de pastas locais que simulam a organiza√ß√£o de um Data Lake a ser replicado no S3 durante o processo de implanta√ß√£o do **terraglue**. O nome do *database* ser√° escolhido aleatoriamente.

```bash
# Criando estrutura de pastas locais
mkdir -p data/tt3/tbl_titanic_data
```

Onde `tt3` √© o nome fict√≠cio para o *database* e `tbl_titanic_data` o nome escolhido para a tabela. Com isso, √© poss√≠vel movimentar o arquivo desejado para a estrutura criada.

```bash
# Movendo arquivo
mv ~/Downloads/titanic.csv data/tt3/tbl_titanic_data/
```

Validado que o novo arquivo j√° consta organizado localmente na estrutura necess√°ria, bastas executar os procedimentos de implanta√ß√£o do **terraglue** atrav√©s do comando `terraform apply` para implementar todos os processos atrelados.

### Visualizando efeitos na conta AWS

Uma vez obtido e organizado o(s) novo(s) conjunto(s) de dado(s) a serem inseridos junto com os processos de implanta√ß√£o do **terraglue**, o usu√°rio poder√° acessar o S3 para avaliar o sucesso de ingest√£o dos novos dados dentro do bucket `terraglue-sor`. Na imagem abaixo, √© poss√≠vel verificar que o conjunto de dados `titanic.csv` foi inserido com sucesso com o prefixo `tt3/tbl_titanic_data`, assim como o esperado.

<details>
  <summary>üì∑ Clique para visualizar a imagem</summary>
  <div align="left">
    <br><img src="https://github.com/ThiagoPanini/terraglue/blob/develop/docs/imgs/examples-cenario03-titanic-data-on-s3.PNG?raw=true" alt="titanic-data-on-s3">
</div>
</details>

Al√©m disso, √© poss√≠vel tamb√©m acessar o servi√ßo Glue e, dentro do menu Data Catalog, validar se o arquivo inserido no S3 tamb√©m passou pelo processo de cataloga√ß√£o. Na imagem abaixo, √© poss√≠vel verificar que uma entrada no cat√°logo foi automaticamente criada para os dados do Titanic rec√©m disponibilizados:

<details>
  <summary>üì∑ Clique para visualizar a imagem</summary>
  <div align="left">
    <br><img src="https://github.com/ThiagoPanini/terraglue/blob/develop/docs/imgs/examples-cenario03-titanic-data-on-data-catalog.PNG?raw=true" alt="titanic-data-on-data-catalog">
</div>
</details>

Ao selecionar a tabela no cat√°logo, ser√° ainda poss√≠vel perceber que todo o processo de obten√ß√£o de atributos p√¥de ser realizado com sucesso. Essa afirma√ß√£o tem como base o pr√≥prio *schema* da tabela presente no cat√°logo:

<details>
  <summary>üì∑ Clique para visualizar a imagem</summary>
  <div align="left">
    <br><img src="https://github.com/ThiagoPanini/terraglue/blob/develop/docs/imgs/examples-cenario03-titanic-schema-on-catalog.PNG?raw=true" alt="titanic-schema-on-data-catalog">
</div>
</details>

Por fim, a valida√ß√£o final realizada envolve o acesso ao servi√ßo Athena para execu√ß√£o de uma simples *query* para extra√ß√£o dos dados rec√©m catalogados. A imagem abaixo exemplifica a retirada de 10 registros da base atrav√©s do comando `SELECT * FROM tt3.tbl_titanic_data LIMIT 10;`:

<details>
  <summary>üì∑ Clique para visualizar a imagem</summary>
  <div align="left">
    <br><img src="https://github.com/ThiagoPanini/terraglue/blob/develop/docs/imgs/examples-cenario03-titanic-data-on-athena.PNG?raw=true" alt="titanic-data-on-athena">
</div>
</details>

Com isso, √© poss√≠vel validar que todo o processo de adapta√ß√£o do **terraglue** para uso de novas bases de dados em substitui√ß√£o aos dados de e-commerce fornecidos por padr√£o pode ser tranquilamente realizado.

___

## Cen√°rio 2: implementando seu pr√≥prio job do Glue

O segundo cen√°rio de exemplos pr√°ticos fornecidos envolve a adapta√ß√£o do script principal `main.py`, fornecido como padr√£o para processamento de dados do E-Commerce Brasileiro, de acordo com necessidades espec√≠ficas do usu√°rio. Visando seguir uma linha sequencial, o processo de adapta√ß√£o da aplica√ß√£o a ser exemplificado considera a utiliza√ß√£o dos dados do Titanic ingeridos e catalogados na exemplifica√ß√£o do [cen√°rio 1 de adapta√ß√£o de dados](#cen√°rio-1-implementando-seu-pr√≥prio-conjunto-de-dados) previamente abordado.

Como tamb√©m detalhado na [documenta√ß√£o espec√≠fica sobre a aplica√ß√£o Spark](https://github.com/ThiagoPanini/terraglue/blob/main/APP.md) entregue ao usu√°rio, existem algumas etapas importantes a serem seguidas para garantir a extra√ß√£o do maior valor poss√≠vel de toda a din√¢mica entregue pelo **terraglue**. Compreender o processo de adapta√ß√£o do script principal de trabalho pode proporcionar as seguintes vantagens ao usu√°rio:

- ‚öóÔ∏è Implantar e testar jobs pr√≥prios do Glue em um ambiente de desenvolvimento
- üî¨ Validar regras de transforma√ß√£o codificadas para gera√ß√£o de tabelas SoT e Spec
- üîß Iterar sobre as regras estabelecidas e se as vis√µes geradas s√£o realmente as esperadas

### Etapas para adapta√ß√£o da aplica√ß√£o

Para o usu√°rio que inseriu novos dados e deseja codificar suas pr√≥prias transforma√ß√µes para testar, validar ou simplesmente entender como um *job* Glue funciona, a lista de t√≥picos abaixo pode servir como um simples resumo das opera√ß√µes necess√°rias:

<details>
  <summary>1. Analisar e modificar, se necess√°rio, a vari√°vel <code>ARGV_LIST</code> presente no script principal para mapear e coletar poss√≠veis novos par√¢metros do job inseridos pelo usu√°rio</summary>
  
  > O processo de inclus√£o de novos par√¢metros pode ser feito atrav√©s da vari√°vel Terraform `glue_job_user_arguments` presente no arquivo `./infra/variables.tf`.
</div>
</details>

<details>
  <summary>2. Modificar, em caso de inclus√£o de novos dados, a vari√°vel <code>DATA_DICT</code> com todas as informa√ß√µes necess√°rias para leitura dos dados a serem trabalhados</summary>

  > Para este processo, todos os argumentos do m√©todo `glueContext.create_dynamic_frame.from_catalog()` s√£o aceitos.
</div>
</details>

<details>
  <summary>3. Codificar novos m√©todos de transforma√ß√£o na classe <code>GlueTransformationManager</code> de acordo com as regras de neg√≥cio a serem aplicadas na gera√ß√£o das novas tabelas</summary>

  > Para fins de organiza√ß√£o, os m√©todos de transforma√ß√£o fornecidos como padr√£o iniciam com o prefixo "transform_". S√£o esses os m√©todos que devem ser substitu√≠dos para o novo processo de ETL codificado.
</div>
</details>

<details>
  <summary>4. Modificar o m√©todo <code>run()</code> da classe `GlueTransformationManager` de acordo com a nova sequ√™nciad e passos necess√°rios at√© o alcance do objetivo final do job</summary>

  > Aqui, o usu√°rio poder√° utilizar todos os m√©todos presentes no script principal e no m√≥dulo `terraglue.py` para coordenar todos os passos e etapas do processo de ETL, desde a leitura dos dados at√© a escrita e cataloga√ß√£o dos mesmos.
</div>
</details>

### Alterando par√¢metros do job

Como um primeiro passo rumo ao processo de adapta√ß√£o da aplica√ß√£o Spark proporcionada pelo **terraglue**, √© importante garantir que todos os par√¢metros do job do Glue est√£o devidamente configurados. Existem diferentes formas de se fazer essa valida√ß√£o, seja alterando diretamente os valores no template de IaC considerado ou mesmo indo manualmente at√© o AWS Management Console da conta de *sandbox* ou de desenvolvimento.

Nessa demonstra√ß√£o, ser√° proposta a altera√ß√£o dos seguintes par√¢metros diretamente nos m√≥dulos Terraform disponibilizados no projeto:

- Nome do job do Glue atrav√©s da vari√°vel `glue_job_name`
- Par√¢metros `--OUTPUT_DB` e `--OUTPUT_TABLE` atrav√©s da vari√°vel `glue_job_user_arguments`

Para isso, basta acessar o arquivo `variables.tf` no m√≥dulo *root*, procurar pelas vari√°veis acima citadas e, enfim, aplicar as modifica√ß√µes desejadas. Visando proporcionar um exemplo pr√°tico desta modifica√ß√£o, o bloco abaixo cont√©m propostas de um novo nome para o *job* modificado, bem como para novos par√¢metros de sa√≠da do banco de dados e da tabela resultante:

<details>
  <summary>üçÉ Vari√°veis glue_job_name e glue_job_user_arguments do arquivo variables.tf no m√≥dulo root </summary>

```
variable "glue_job_name" {
  description = "Nome ou refer√™ncia do job do glue a ser criado"
  type        = string
  default     = "gluejob-sot-titanic"
}

[...]

variable "glue_job_user_arguments" {
  description = "Conjunto de argumentos personalizados do usu√°rio a serem associados ao job do glue"
  type        = map(string)
  default = {
    "--OUTPUT_DB"             = "tt3"
    "--OUTPUT_TABLE"          = "tbsot_titanic"
    "--CONNECTION_TYPE"       = "s3"
    "--UPDATE_BEHAVIOR"       = "UPDATE_IN_DATABASE"
    "--PARTITION_NAME"        = "anomesdia"
    "--PARTITION_FORMAT"      = "%Y%m%d"
    "--DATA_FORMAT"           = "parquet"
    "--COMPRESSION"           = "snappy"
    "--ENABLE_UPDATE_CATALOG" = "True"
  }
}
```
</details>

Dessa forma, ao executar o comando `terraform apply`, o usu√°rio poder√° ter em m√£os um job com a nomenclatura correta e com os par√¢metros configurados de maneira a proporcionar a escrita de uma tabela totalmente nova.

### Modificando o dicion√°rio DATA_DICT

Refor√ßando a considera√ß√£o de que os novos dados a serem trabalhados nesta adapta√ß√£o do script principal foram previamente inseridos e catalogados no [exemplo de cen√°rio anterior](#cen√°rio-1-implementando-seu-pr√≥prio-conjunto-de-dados), o in√≠cio do nosso processo de adapta√ß√£o envolve modificar a vari√°vel `DATA_DICT` no script `main.py` para inserir os par√¢metros de leitura da tabela do Titanic presente agora no processo. Dessa forma, a nova vari√°vel √© dada por:

```python
# Definindo dicion√°rio para mapeamento dos dados
DATA_DICT = {
    "titanic": {
        "database": "tt3",
        "table_name": "tbl_titanic_data",
        "transformation_ctx": "dyf_titanic",
        "create_temp_view": True
    }
}
```

Os valores inseridos na vari√°vel `DATA_DICT` correspondem √†s entradas existentes no cat√°logo de dados para a dada tabela. Como estamos realizando a leitura de apenas uma origem, o dicion√°rio √© composto apenas por uma chave.

### Codificando novos m√©todos de transforma√ß√£o

Agora que o dicion√°rio de mapeamento de leitura de dados est√° devidamente configurado, vamos estabelecer um objetivo final para garantir que as inclus√µes dos m√©todos de transforma√ß√£o da classe `GlueTransformationManager` tenham um prop√≥sito claro. Antes de formalizar uma proposta, √© importante ter uma vis√£o pr√©via sobre os dados atualmente dispon√≠veis:

<details>
  <summary>üì∑ Clique para visualizar a imagem</summary>
  <div align="left">
    <br><img src="https://raw.githubusercontent.com/ThiagoPanini/terraglue/develop/docs/imgs/terraglue-examples-titanic-data.png" alt="titanic-data-athena">
</div>
</details>

Considerando o conte√∫do da base Titanic presente, a proposta de transforma√ß√£o poderia envolver:

- Transforma√ß√£o de tipos primitivos das colunas de acordo com o significado de cada campo
- Extra√ß√£o do "t√≠tulo" da pessoa (Mr ou Mrs) atrav√©s da coluna *name*
- Extra√ß√£o da "classe da cabine" (A, B ou C) atrav√©s da coluna *cabin*
- Cria√ß√£o de categoria de idade para separa√ß√£o da coluna *age* em faixas
- Cria√ß√£o de categoria de ganhos para separa√ß√£o da coluna *fare* em faixas
- Extra√ß√£o do "tamanho da fam√≠lia" atrav√©s da soma das colunas *parch* e *sibsp*

Assim, a classe `GlueTransformationManager` pode conter ent√£o o m√©todo `transform_titanic()` seguindo as transforma√ß√µes mapeadas atrav√©s do seguinte c√≥digo:

```python
# M√©todo de transforma√ß√£o: payments
def transform_titanic(self, df: DataFrame) -> DataFrame:
    logger.info("Preparando DAG de transforma√ß√µes para a base titanic")
    try:
        # Selecionando e transformando atributos
        df_titanic_select = df.selectExpr(
            "cast(passengerid AS INT) AS id_passageiro",
            "cast(survived AS INT) AS flag_sobrevivencia",
            "cast(pclass AS INT) AS classe_passageiro",
            "name AS nome_passageiro",
            "sex AS genero_passageiro",
            "cast(age AS INT) AS idade_passageiro",
            "cast(sibsp AS INT) AS qtd_irmaos_ou_conjuges",
            "cast(parch AS INT) AS qtd_pais_ou_criancas",
            "ticket",
            "cast(fare AS DECIMAL(17,2)) AS vlr_ticket_pago",
            "cabin AS cabine_passageiro",
            "embarked AS codigo_porto_embarque"
        )

        # Criando atributos adicionais
        df_titanic_prep = df_titanic_select.selectExpr(
            "*",
            "regexp_extract(nome_passageiro, '([a-zA-Z]+\.)') AS titulo_passageiro",
            "lpad(cabine_passageiro, 1, ' ') AS classe_cabine_passageiro,"
            "case\
                when idade_passageiro <= 10 then '0_10'\
                when idade_passageiro <= 20 then '10_20'\
                when idade_passageiro <= 40 then '20_40'\
                when idade_passageiro <= 60 then '40_60'\
                when idade_passageiro > 60 then 'maior_60'\
                else null\
            end AS categoria_idade",
            "case\
                when vlr_ticket_pago <= 8 then '0_8'\
                when vlr_ticket_pago <= 15 then '8_15'\
                when vlr_ticket_pago <= 25 then '15_25'\
                when vlr_ticket_pago <= 50 then '25_50'\
                when vlr_ticket_pago > 50 then 'maior_50'\
                else null\
            end AS categoria_vlr_ticket",
            "qtd_irmaos_ou_conjuges + qtd_pais_ou_criancas + 1 AS tamanho_da_familia"
        )

        # Retornando DataFrame preparado
        return df_titanic_prep

    except Exception as e:
        logger.error("Erro ao preparar DAG de transforma√ß√µes para dados "
                      f"do Titanic. Exception: {e}")
        raise e
```

### Sequenciando passos no m√©todo run()

Uma vez preparado o m√©todo de transforma√ß√£o dos dados do Titanic utilizando `pyspark`, podemos navegar pelo m√©todo `run()` e mapear todos os passos necess√°rios para sequenciamento das etapas. Considerando os objetivos propostos, queremos:

1. Realizar a leitura da base de dados em um DataFrame Spark
2. Aplicar o m√©todo de transforma√ß√£o codificado de modo a gerar um DataFrame transformado
3. Gerenciar parti√ß√µes (eliminar existente e adicionar nova com base em data de execu√ß√£o)
4. Escrever a tabela resultante no S3 e realizar a cataloga√ß√£o no Data Catalog

Com os passos mapeados acima, o m√©todo `run()` pode ser escrito como:

```python
# Encapsulando m√©todo √∫nico para execu√ß√£o do job
def run(self) -> None:
    # Preparando insumos do job
    job = self.init_job()

    # Lendo DynamicFrames e transformando em DataFrames Spark
    dfs_dict = self.generate_dataframes_dict()

    # Separando DataFrames em vari√°veis
    df_titanic = dfs_dict["titanic"]

    # Transformando dados
    df_titanic_prep = self.transform_titanic(df=df_titanic)

    # Criando vari√°vel de parti√ß√£o
    partition_value = int(datetime.now().strftime(
        self.args["PARTITION_FORMAT"]
    ))

    # Removendo parti√ß√£o f√≠sica do S3
    self.drop_partition(
        partition_name=self.args["PARTITION_NAME"],
        partition_value=partition_value
    )

    # Adicionando coluna de parti√ß√£o ao DataFrame
    df_titanic_prep_partitioned = self.add_partition(
        df=df_titanic_prep,
        partition_name=self.args["PARTITION_NAME"],
        partition_value=partition_value
    )

    # Escrevendo e catalogando dados
    self.write_data_to_catalog(df=df_titanic_prep_partitioned)

    # Commitando job
    job.commit()
```

Caso queira visualizar o script completo, basta expandir o bloco abaixo.

<details>
  <summary>üêç Script main.py completo ap√≥s as modifica√ß√µes</summary>

```python
"""
JOB: main.py

CONTEXTO:
---------
Script principal da aplica√ß√£o Spark implantada como job do
Glue dentro dos contextos estabelecidos pelo processo de
ETL a ser programado.
------------------------------------------------------

------------------------------------------------------
---------- 1. PREPARA√á√ÉO INICIAL DO SCRIPT -----------
          1.1 Importa√ß√£o das bibliotecas
---------------------------------------------------"""

# Bibliotecas utilizadas na constru√ß√£o do m√≥dulo
from datetime import datetime
from pyspark.sql import DataFrame
from terraglue import GlueETLManager, log_config


"""---------------------------------------------------
---------- 1. PREPARA√á√ÉO INICIAL DO SCRIPT -----------
        1.2 Definindo vari√°veis da aplica√ß√£o
---------------------------------------------------"""

# Configurando objeto de log
logger = log_config(logger_name=__file__)

# Argumentos do job
ARGV_LIST = [
    "JOB_NAME",
    "OUTPUT_BUCKET",
    "OUTPUT_DB",
    "OUTPUT_TABLE",
    "CONNECTION_TYPE",
    "UPDATE_BEHAVIOR",
    "PARTITION_NAME",
    "PARTITION_FORMAT",
    "DATA_FORMAT",
    "COMPRESSION",
    "ENABLE_UPDATE_CATALOG"
]

# Definindo dicion√°rio para mapeamento dos dados
DATA_DICT = {
    "titanic": {
        "database": "tt3",
        "table_name": "tbl_titanic_data",
        "transformation_ctx": "dyf_titanic",
        "create_temp_view": True
    }
}


"""---------------------------------------------------
--------- 2. GERENCIAMENTO DE TRANSFORMA√á√ïES ---------
            2.2 Defini√ß√£o de classe Python
---------------------------------------------------"""


class GlueTransformationManager(GlueETLManager):
    """
    Classe respons√°vel por gerenciar e fornecer m√©todos t√≠picos
    de transforma√ß√£o de um job do Glue a serem pontualmente
    adaptados por seus usu√°rios para que as opera√ß√µes nos dados
    possam ser aplicadas de acordo com as necessidades exigidas.

    Em ess√™ncia, essa classe herda os atributos e m√©todos da
    classe GlueETLManager existente no m√≥dulo terraglue.py,
    permitindo assim o acesso a todos os atributos e m√©todos
    necess√°rios para inicializa√ß√£o e configura√ß√£o de um job do Glue.
    Assim, basta que o usu√°rio desenvolva os m√©todos de
    transforma√ß√£o adequados para seu processo de ETL e coordene
    a execu√ß√£o dos mesmos no m√©todo run() desta classe.

    Para maiores informa√ß√µes sobre os atributos, basta consultar
    a documenta√ß√£o das classes e m√©todos no m√≥dulo terraglue.py.
    """

    def __init__(self, argv_list: list, data_dict: dict) -> None:
        self.argv_list = argv_list
        self.data_dict = data_dict

        # Herdando atributos de classe de gerenciamento de job
        GlueETLManager.__init__(self, argv_list=self.argv_list,
                                data_dict=self.data_dict)

    # M√©todo de transforma√ß√£o: payments
    def transform_titanic(self, df: DataFrame) -> DataFrame:
        """
        M√©todo de transforma√ß√£o espec√≠fico para uma das origens
        do job do Glue.

        Par√¢metros
        ----------
        :param: df
            DataFrame Spark alvo das transforma√ß√µes aplicadas.
            [type: pyspark.sql.DataFrame]

        Retorno
        -------
        :return: df_prep
            Elemento do tipo DataFrame Spark ap√≥s as transforma√ß√µes
            definidas pelos m√©todos aplicadas dentro da DAG.
            [type: DataFrame]
        """

        logger.info("Preparando DAG de transforma√ß√µes para a base titanic")
        try:
            # Selecionando e transformando atributos
            df_titanic_select = df.selectExpr(
                "cast(passengerid AS INT) AS id_passageiro",
                "cast(survived AS INT) AS flag_sobrevivencia",
                "cast(pclass AS INT) AS classe_passageiro",
                "name AS nome_passageiro",
                "sex AS genero_passageiro",
                "cast(age AS INT) AS idade_passageiro",
                "cast(sibsp AS INT) AS qtd_irmaos_ou_conjuges",
                "cast(parch AS INT) AS qtd_pais_ou_criancas",
                "ticket",
                "cast(fare AS DECIMAL(17,2)) AS vlr_ticket_pago",
                "cabin AS cabine_passageiro",
                "embarked AS codigo_porto_embarque"
            )

            # Criando atributos adicionais
            df_titanic_prep = df_titanic_select.selectExpr(
                "*",
                "regexp_extract(nome_passageiro, '([a-zA-Z]+\.)') AS titulo_passageiro",
                "lpad(cabine_passageiro, 1, ' ') AS classe_cabine_passageiro,"
                "case\
                    when idade_passageiro <= 10 then '0_10'\
                    when idade_passageiro <= 20 then '10_20'\
                    when idade_passageiro <= 40 then '20_40'\
                    when idade_passageiro <= 60 then '40_60'\
                    when idade_passageiro > 60 then 'maior_60'\
                    else null\
                end AS categoria_idade",
                "case\
                    when vlr_ticket_pago <= 8 then '0_8'\
                    when vlr_ticket_pago <= 15 then '8_15'\
                    when vlr_ticket_pago <= 25 then '15_25'\
                    when vlr_ticket_pago <= 50 then '25_50'\
                    when vlr_ticket_pago > 50 then 'maior_50'\
                    else null\
                end AS categoria_vlr_ticket",
                "qtd_irmaos_ou_conjuges + qtd_pais_ou_criancas + 1 AS tamanho_da_familia"
            )

            # Retornando DataFrame preparado
            return df_titanic_prep

        except Exception as e:
            logger.error("Erro ao preparar DAG de transforma√ß√µes para dados "
                        f"do Titanic. Exception: {e}")
            raise e

    # Encapsulando m√©todo √∫nico para execu√ß√£o do job
    def run(self) -> None:
        """
        M√©todo respons√°vel por consolidar todas as etapas de execu√ß√£o
        do job do Glue, permitindo assim uma maior facilidade e
        organiza√ß√£o ao usu√°rio final. Este m√©todo pode ser devidamente
        adaptado de acordo com as necessidades de cada usu√°rio e de
        cada job a ser codificado, possibilitando uma centraliza√ß√£o
        de todos os processos operacionais a serem realizados.
        Na pr√°tica, este m√©todo realiza as seguintes opera√ß√µes:

            1. Inicializa o job e obt√©m todos os insumos necess√°rios
            2. Realiza a leitura dos objetos DataFrame/DynamicFrame
            3. Aplica as transforma√ß√µes necess√°rias
            4. Gerencia parti√ß√µes (elimina existente e adiciona uma nova)
            5. Escreve o resultado no s3 e cataloga no Data Catalog
        """

        # Preparando insumos do job
        job = self.init_job()

        # Lendo DynamicFrames e transformando em DataFrames Spark
        dfs_dict = self.generate_dataframes_dict()

        # Separando DataFrames em vari√°veis
        df_titanic = dfs_dict["titanic"]

        # Transformando dados
        df_titanic_prep = self.transform_titanic(df=df_titanic)

        # Criando vari√°vel de parti√ß√£o
        partition_value = int(datetime.now().strftime(
            self.args["PARTITION_FORMAT"]
        ))

        # Removendo parti√ß√£o f√≠sica do S3
        self.drop_partition(
            partition_name=self.args["PARTITION_NAME"],
            partition_value=partition_value
        )

        # Adicionando coluna de parti√ß√£o ao DataFrame
        df_titanic_prep_partitioned = self.add_partition(
            df=df_titanic_prep,
            partition_name=self.args["PARTITION_NAME"],
            partition_value=partition_value
        )

        # Escrevendo e catalogando dados
        self.write_data_to_catalog(df=df_titanic_prep_partitioned)

        # Commitando job
        job.commit()


"""---------------------------------------------------
--------------- 4. PROGRAMA PRINCIPAL ----------------
        Execu√ß√£o do job a partir de classes
---------------------------------------------------"""

if __name__ == "__main__":

    # Inicializando objeto para gerenciar o job e as transforma√ß√µes
    glue_manager = GlueTransformationManager(
        argv_list=ARGV_LIST,
        data_dict=DATA_DICT
    )

    # Executando todas as l√≥gicas mapeadas do job
    glue_manager.run()
```

</details>


### Visualizando resultados

Ap√≥s a completa adapta√ß√£o do script e execu√ß√£o do job no Glue, o usu√°rio ter√° em m√£os uma nova tabela SoT (no exemplo, chamada `tbsot_titanic`) com novos dados dispon√≠veis para uso.

Considerando a demonstra√ß√£o fornecida, seria poss√≠vel acessar o servi√ßo Athena e visualizar, logo de cara, uma nova entrada para a tabela gerada no database selecionado:

<details>
  <summary>üì∑ Clique para visualizar a imagem</summary>
  <div align="left">
    <br><img src="https://raw.githubusercontent.com/ThiagoPanini/terraglue/develop/docs/imgs/terraglue-examples-titanic-sot-athena.png" alt="titanic-data-athena-sot">
</div>
</details>

Para validar as transforma√ß√µes codificadas, o usu√°rio poderia, ainda, executar a query abaixo para visualizar os novos dados dispon√≠veis.

```sql
SELECT
    idade_passageiro,
    categoria_idade,
    vlr_ticket_pago,
    categoria_vlr_ticket,
    tamanho_da_familia,
    cabine_passageiro,
    classe_cabine_passageiro

FROM tt3.tbsot_titanic LIMIT 5;
```

<details>
  <summary>üì∑ Clique para visualizar a imagem</summary>
  <div align="left">
    <br><img src="https://raw.githubusercontent.com/ThiagoPanini/terraglue/develop/docs/imgs/terraglue-examples-titanic-sot-athena-query.png" alt="titanic-data-athena-sot-query">
</div>
</details>

E assim completamos o cen√°rio de adapta√ß√£o do script `main.py` para finalidades espec√≠ficas de acordo com novos dados inseridos no processo!

___

Continue sua jornada no **terraglue** atrav√©s das documenta√ß√µes!

- [1. Documenta√ß√£o principal do projeto](https://github.com/ThiagoPanini/terraglue/tree/main)
- [2. Instala√ß√£o e primeiros passos](https://github.com/ThiagoPanini/terraglue/blob/main/GETTINGSTARTED.md) 
- [3. Infraestrutura provisionada](https://github.com/ThiagoPanini/terraglue/blob/main/INFRA.md) 
- [4. Uma proposta de padroniza√ß√£o de jobs Glue](https://github.com/ThiagoPanini/terraglue/blob/main/APP.md) 
- üëâ [5. Exemplos pr√°ticos de utiliza√ß√£o da solu√ß√£o](https://github.com/ThiagoPanini/terraglue/blob/main/EXAMPLES.md) *Voc√™ est√° aqui!*